{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using Unsupervised Learning Models\n",
    "###              -- Final Project for LING 6300 Machine Learning & Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large amounts of data are collected everyday. As more information becomes available, it becomes difficult to access what we are looking for. So, we need tools and techniques to organize, search and understand vast quantities of information. Topic Modeling is one of these techniques.\n",
    "\n",
    "Topic modeling is a type of algorithm that scans a set of documents (known in the NLP field as a corpus), examines how words and phrases co-occur in them, and automatically “learns” groups or clusters of words that best characterize those documents.(Patrick van Kessel, 2018) These sets of words often appear to represent a coherent theme or topic.\n",
    "\n",
    "In my final project, I will use two unsupervised learning models: K-Means and Latent Dirichlet Allocation(LDA) to do topic modeling on a user reviews dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages and libraries I used for my project include: (1) Numpy; (2) Pandas; (3) nltk; (4) gensim; (5) Scikit-learn; (6) warnings. (7) matplotlib\n",
    "\n",
    "(1) Install python version 3: https://www.python.org/downloads/\n",
    "\n",
    "(2) Install packages that's not on your computer yet: pip3 install PACKAGE NAME\n",
    "\n",
    "(3) Put the data file reviews.tsv in the same folder with this jupyter notenook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lansang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lansang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I used is a user reviews dataset about watches from Amazon. It is a open sourced dataset which can be downloaded at : https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz (I already put this data file as reviews.tsv in the same folder with this jupyter file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8704: expected 15 fields, saw 22\\nSkipping line 16933: expected 15 fields, saw 22\\nSkipping line 23726: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 85637: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 132136: expected 15 fields, saw 22\\nSkipping line 158070: expected 15 fields, saw 22\\nSkipping line 166007: expected 15 fields, saw 22\\nSkipping line 171877: expected 15 fields, saw 22\\nSkipping line 177756: expected 15 fields, saw 22\\nSkipping line 181773: expected 15 fields, saw 22\\nSkipping line 191085: expected 15 fields, saw 22\\nSkipping line 196273: expected 15 fields, saw 22\\nSkipping line 196331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 197000: expected 15 fields, saw 22\\nSkipping line 197011: expected 15 fields, saw 22\\nSkipping line 197432: expected 15 fields, saw 22\\nSkipping line 208016: expected 15 fields, saw 22\\nSkipping line 214110: expected 15 fields, saw 22\\nSkipping line 244328: expected 15 fields, saw 22\\nSkipping line 248519: expected 15 fields, saw 22\\nSkipping line 254936: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 272057: expected 15 fields, saw 22\\nSkipping line 293214: expected 15 fields, saw 22\\nSkipping line 310507: expected 15 fields, saw 22\\nSkipping line 312306: expected 15 fields, saw 22\\nSkipping line 316296: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 336028: expected 15 fields, saw 22\\nSkipping line 344885: expected 15 fields, saw 22\\nSkipping line 352551: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 408773: expected 15 fields, saw 22\\nSkipping line 434535: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 581593: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 652409: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reviews.tsv', sep = '\\t', header = 0, error_bad_lines=False) #Read in data file. Skip error data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>3653882</td>\n",
       "      <td>R3O9SGZBVQBV76</td>\n",
       "      <td>B00FALQ1ZC</td>\n",
       "      <td>937001370</td>\n",
       "      <td>Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Absolutely love this watch! Get compliments al...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>14661224</td>\n",
       "      <td>RKH8BNC3L5DLF</td>\n",
       "      <td>B00D3RGO20</td>\n",
       "      <td>484010722</td>\n",
       "      <td>Kenneth Cole New York Women's KC4944 Automatic...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I love thiswatch it keeps time wonderfully</td>\n",
       "      <td>I love this watch it keeps time wonderfully.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>27324930</td>\n",
       "      <td>R2HLE8WKZSU3NL</td>\n",
       "      <td>B00DKYC7TK</td>\n",
       "      <td>361166390</td>\n",
       "      <td>Ritche 22mm Black Stainless Steel Bracelet Wat...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>Scratches</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>7211452</td>\n",
       "      <td>R31U3UH5AZ42LL</td>\n",
       "      <td>B000EQS1JW</td>\n",
       "      <td>958035625</td>\n",
       "      <td>Citizen Men's BM8180-03E Eco-Drive Stainless S...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>It works well on me. However, I found cheaper ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>12733322</td>\n",
       "      <td>R2SV659OUJ945Y</td>\n",
       "      <td>B00A6GFD7S</td>\n",
       "      <td>765328221</td>\n",
       "      <td>Orient ER27009B Men's Symphony Automatic Stain...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Beautiful face, but cheap sounding links</td>\n",
       "      <td>Beautiful watch face.  The band looks nice all...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US      3653882  R3O9SGZBVQBV76  B00FALQ1ZC       937001370   \n",
       "1          US     14661224   RKH8BNC3L5DLF  B00D3RGO20       484010722   \n",
       "2          US     27324930  R2HLE8WKZSU3NL  B00DKYC7TK       361166390   \n",
       "3          US      7211452  R31U3UH5AZ42LL  B000EQS1JW       958035625   \n",
       "4          US     12733322  R2SV659OUJ945Y  B00A6GFD7S       765328221   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...          Watches   \n",
       "1  Kenneth Cole New York Women's KC4944 Automatic...          Watches   \n",
       "2  Ritche 22mm Black Stainless Steel Bracelet Wat...          Watches   \n",
       "3  Citizen Men's BM8180-03E Eco-Drive Stainless S...          Watches   \n",
       "4  Orient ER27009B Men's Symphony Automatic Stain...          Watches   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            5              0            0    N                 Y   \n",
       "1            5              0            0    N                 Y   \n",
       "2            2              1            1    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            4              0            0    N                 Y   \n",
       "\n",
       "                              review_headline  \\\n",
       "0                                  Five Stars   \n",
       "1  I love thiswatch it keeps time wonderfully   \n",
       "2                                   Two Stars   \n",
       "3                                  Five Stars   \n",
       "4    Beautiful face, but cheap sounding links   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  Absolutely love this watch! Get compliments al...  2015-08-31  \n",
       "1       I love this watch it keeps time wonderfully.  2015-08-31  \n",
       "2                                          Scratches  2015-08-31  \n",
       "3  It works well on me. However, I found cheaper ...  2015-08-31  \n",
       "4  Beautiful watch face.  The band looks nice all...  2015-08-31  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()# Take a look at how the dataset looks like. The most inportant column for this project is review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_body.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the first 3000 reviews so that it won't take too long to train the model.\n",
    "data = df.loc[:3000, 'review_body'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used two main techniques in data preprocessing: Tokenization and Stemming. Tokenization is taking a text or set of text and breaking it up into its individual words.(Tatman, 2017) Stemming refers to the process of breaking a word down into its root. For example, if we have a a sentence: The boy’s cars are different colors. After stemming, We should get : The boy car be differ color. I also use the stopwords function in the nltk package so that we can ignore words like a, an, the, etc.\n",
    "\n",
    "However, Stemming has some disadvantages such as overstemming and understemming. So I also tried tokenization without stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use 179 stop words from nltk library\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "#Use the stopwords function in nltk package.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print('We use ' + str(len(stopwords)) + ' stop words from nltk library')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "#Tokenization with stemming\n",
    "def tokenization_and_stemming(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "# tokenization without stemming\n",
    "def tokenization(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolut',\n",
       " 'love',\n",
       " 'watch',\n",
       " 'get',\n",
       " 'compliment',\n",
       " 'almost',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'i',\n",
       " 'wear',\n",
       " 'dainti']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one example after being stemmed\n",
    "tokenization_and_stemming(data[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. I did tokenization and stemming for all the documents\n",
    "# 2. I also tried to just do tokenization for all the documents without stemming\n",
    "docs_stemmed = []\n",
    "docs_tokenized = []\n",
    "for i in data:\n",
    "    tokenized_and_stemmed_results = tokenization_and_stemming(i)\n",
    "    docs_stemmed.extend(tokenized_and_stemmed_results)\n",
    "    \n",
    "    tokenized_results = tokenization(i)\n",
    "    docs_tokenized.extend(tokenized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from stemmed words to original tokenized words for result interpretation.\n",
    "vocab_frame_dict = {docs_stemmed[x]:docs_tokenized[x] for x in range(len(docs_stemmed))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I used tf-idf to determine the importance of words in the documents.TF-IDF, full name is term frequency and inverse document frequency. It is the product of statistics, term frequency and inverse document frequency. It can be used as a weighting factor to reflect  how important a word is to a document in a corpus. Term Frequency refers to count of word A in document B. The weight of a term that occurs in a document is simply proportional to the term frequency.\n",
    "\n",
    "The document frequency means number of documents where word A appears so inverse document frequency = 1/ (number of documents where word A appears) . It will diminish the weight of terms that occur very frequently in the document set and increase the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the TfidfVectorizer from scikit-learn package and define vectorizer parameters\n",
    "# max_df : maximum document frequency for the given word\n",
    "# min_df : minimum document frequency for the given word\n",
    "# max_features: maximum number of words\n",
    "# use_idf: if not true, we only calculate tf\n",
    "tfidf_model = TfidfVectorizer(max_df = 0.99, max_features = 1000,\n",
    "                                 min_df = 0.01, stop_words = 'english',\n",
    "                                 use_idf = True, tokenizer = tokenization_and_stemming, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 2998 reviews and 254 terms.\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_model.fit_transform(data)\n",
    "\n",
    "print('In total, there are ' + str(tfidf_matrix.shape[0]) + \\\n",
    "    ' reviews and ' + str(tfidf_matrix.shape[1]) + ' terms.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 0.99,\n",
       " 'max_features': 1000,\n",
       " 'min_df': 0.01,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': <function __main__.tokenization_and_stemming(text)>,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check parameters\n",
    "tfidf_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_words = tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'m\",\n",
       " \"'s\",\n",
       " 'abl',\n",
       " 'absolut',\n",
       " 'accur',\n",
       " 'actual',\n",
       " 'adjust',\n",
       " 'alarm',\n",
       " 'alreadi',\n",
       " 'alway',\n",
       " 'amaz',\n",
       " 'amazon',\n",
       " 'anoth',\n",
       " 'appear',\n",
       " 'arm',\n",
       " 'arriv',\n",
       " 'attract',\n",
       " 'automat',\n",
       " 'awesom',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'batteri',\n",
       " 'beauti',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bezel',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'blue',\n",
       " 'bought',\n",
       " 'box',\n",
       " 'br',\n",
       " 'bracelet',\n",
       " 'brand',\n",
       " 'break',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'ca',\n",
       " 'came',\n",
       " 'case',\n",
       " 'casio',\n",
       " 'chang',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'clasp',\n",
       " 'classi',\n",
       " 'clear',\n",
       " 'clock',\n",
       " 'collect',\n",
       " 'color',\n",
       " 'come',\n",
       " 'comfort',\n",
       " 'compliment',\n",
       " 'cool',\n",
       " 'cost',\n",
       " 'coupl',\n",
       " 'crown',\n",
       " 'crystal',\n",
       " 'cute',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'deal',\n",
       " 'definit',\n",
       " 'deliveri',\n",
       " 'design',\n",
       " 'dial',\n",
       " 'differ',\n",
       " 'difficult',\n",
       " 'digit',\n",
       " 'disappoint',\n",
       " 'display',\n",
       " 'dress',\n",
       " 'durabl',\n",
       " 'easi',\n",
       " 'easili',\n",
       " 'eleg',\n",
       " 'end',\n",
       " 'everi',\n",
       " 'everyday',\n",
       " 'everyth',\n",
       " 'exact',\n",
       " 'excel',\n",
       " 'expect',\n",
       " 'expens',\n",
       " 'face',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'featur',\n",
       " 'feel',\n",
       " 'fell',\n",
       " 'figur',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'fit',\n",
       " 'function',\n",
       " 'gave',\n",
       " 'gift',\n",
       " 'glass',\n",
       " 'goe',\n",
       " 'gold',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'hand',\n",
       " 'happi',\n",
       " 'hard',\n",
       " 'heavi',\n",
       " 'help',\n",
       " 'high',\n",
       " 'hold',\n",
       " 'honest',\n",
       " 'hope',\n",
       " 'hour',\n",
       " 'howev',\n",
       " 'husband',\n",
       " 'includ',\n",
       " 'instruct',\n",
       " 'invicta',\n",
       " 'issu',\n",
       " 'item',\n",
       " 'job',\n",
       " 'know',\n",
       " 'larg',\n",
       " 'larger',\n",
       " 'leather',\n",
       " 'light',\n",
       " 'like',\n",
       " 'link',\n",
       " 'littl',\n",
       " 'll',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'low',\n",
       " 'make',\n",
       " 'mani',\n",
       " 'manual',\n",
       " 'metal',\n",
       " 'minut',\n",
       " 'model',\n",
       " 'money',\n",
       " 'month',\n",
       " 'movement',\n",
       " \"n't\",\n",
       " 'need',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'notic',\n",
       " 'number',\n",
       " 'old',\n",
       " 'open',\n",
       " 'order',\n",
       " 'origin',\n",
       " 'overal',\n",
       " 'packag',\n",
       " 'pay',\n",
       " 'peopl',\n",
       " 'perfect',\n",
       " 'person',\n",
       " 'pictur',\n",
       " 'piec',\n",
       " 'pin',\n",
       " 'place',\n",
       " 'plastic',\n",
       " 'pleas',\n",
       " 'point',\n",
       " 'poor',\n",
       " 'pretti',\n",
       " 'price',\n",
       " 'probabl',\n",
       " 'problem',\n",
       " 'product',\n",
       " 'purchas',\n",
       " 'qualiti',\n",
       " 'quick',\n",
       " 'quit',\n",
       " 'rate',\n",
       " 'read',\n",
       " 'real',\n",
       " 'realli',\n",
       " 'reason',\n",
       " 'receiv',\n",
       " 'recommend',\n",
       " 'remov',\n",
       " 'replac',\n",
       " 'resist',\n",
       " 'return',\n",
       " 'review',\n",
       " 'right',\n",
       " 'run',\n",
       " 'said',\n",
       " 'say',\n",
       " 'scratch',\n",
       " 'second',\n",
       " 'seiko',\n",
       " 'seller',\n",
       " 'send',\n",
       " 'set',\n",
       " 'sever',\n",
       " 'ship',\n",
       " 'short',\n",
       " 'simpl',\n",
       " 'sinc',\n",
       " 'size',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'solid',\n",
       " 'someth',\n",
       " 'son',\n",
       " 'star',\n",
       " 'start',\n",
       " 'stop',\n",
       " 'strap',\n",
       " 'style',\n",
       " 'stylish',\n",
       " 'sure',\n",
       " 'swim',\n",
       " 'swiss',\n",
       " 'tell',\n",
       " 'thank',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'timex',\n",
       " 'took',\n",
       " 'tri',\n",
       " 'turn',\n",
       " 'use',\n",
       " 'valu',\n",
       " 've',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'way',\n",
       " 'wear',\n",
       " 'week',\n",
       " 'weight',\n",
       " 'went',\n",
       " 'white',\n",
       " 'wife',\n",
       " 'wish',\n",
       " 'work',\n",
       " 'worn',\n",
       " 'worth',\n",
       " 'wrist',\n",
       " 'year']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1 Calculate document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tried to calculate the document similarity. This is not required for topic modeling. I did it just because I already got the tfidf_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.42208339 0.         ... 0.11382006 0.04257457 0.        ]\n",
      " [0.42208339 1.         0.         ... 0.26966248 0.10086768 0.        ]\n",
      " [0.         0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.11382006 0.26966248 0.         ... 1.         0.10329327 0.        ]\n",
      " [0.04257457 0.10086768 0.         ... 0.10329327 1.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Try to calculate the document similarity. \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_matrix = cosine_similarity(tfidf_matrix)\n",
    "print(cos_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I used is K-means clustering(James MacQueen, 1967). K-means clustering is an unsupervised learning model. It aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. Here I tried some numbers and finally set 5 as the cluster number. I used the K-means from scikit-learn library to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_num = 5\n",
    "km = KMeans(n_clusters = clusters_num)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kenneth Cole New York Women's KC4944 Automatic...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ritche 22mm Black Stainless Steel Bracelet Wat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citizen Men's BM8180-03E Eco-Drive Stainless S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orient ER27009B Men's Symphony Automatic Stain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Casio Men's GW-9400BJ-1JF G-Shock Master of G ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fossil Women's ES3851 Urban Traveler Multifunc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INFANTRY Mens Night Vision Analog Quartz Wrist...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>G-Shock Men's Grey Sport Watch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Heiden Quad Watch Winder in Black Leather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  cluster\n",
       "0  Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...        4\n",
       "1  Kenneth Cole New York Women's KC4944 Automatic...        4\n",
       "2  Ritche 22mm Black Stainless Steel Bracelet Wat...        0\n",
       "3  Citizen Men's BM8180-03E Eco-Drive Stainless S...        0\n",
       "4  Orient ER27009B Men's Symphony Automatic Stain...        0\n",
       "5  Casio Men's GW-9400BJ-1JF G-Shock Master of G ...        0\n",
       "6  Fossil Women's ES3851 Urban Traveler Multifunc...        3\n",
       "7  INFANTRY Mens Night Vision Analog Quartz Wrist...        0\n",
       "8                     G-Shock Men's Grey Sport Watch        3\n",
       "9          Heiden Quad Watch Winder in Black Leather        0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame and check 10 results\n",
    "product = {'review': df[:2998].product_title, 'cluster': clusters}\n",
    "frame = pd.DataFrame(product, columns = ['review', 'cluster'])\n",
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews included in each cluster:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster\n",
       "0     2076\n",
       "3      300\n",
       "4      269\n",
       "2      191\n",
       "1      162"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of reviews in each cluster. \n",
    "#We can see that it is not balanced. The number of reviews in cluster 0 is larger than the combination number of all the other four clusters\n",
    "print('Number of reviews included in each cluster:')\n",
    "frame['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document clustering result by K-Means>\n",
      "Cluster0 word:  watches, like, looks, 's, br, works, \n",
      "Cluster0reviews (2076reviews): \n",
      "Cluster1 word:  good, watches, looks, quality, product, price, \n",
      "Cluster1reviews (162reviews): \n",
      "Cluster2 word:  nice, watches, price, looks, expected, really, \n",
      "Cluster2reviews (191reviews): \n",
      "Cluster3 word:  greatly, watches, looks, price, works, product, \n",
      "Cluster3reviews (300reviews): \n",
      "Cluster4 word:  loves, watches, gifts, beautiful, wife, absolutely, \n",
      "Cluster4reviews (269reviews): \n"
     ]
    }
   ],
   "source": [
    "#Print cluster word(6 in each) and reviews for each cluster\n",
    "print('<Document clustering result by K-Means>')\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "cluster_keywords_summary = {}\n",
    "for i in range(clusters_num):\n",
    "    print('Cluster' + str(i) + ' word: ', end = ' ')\n",
    "    cluster_keywords_summary[i] = []\n",
    "    for n in order_centroids[i, :6]:\n",
    "        cluster_keywords_summary[i].append(vocab_frame_dict[tf_words[n]])\n",
    "        print(vocab_frame_dict[tf_words[n]] + ',', end = ' ')\n",
    "    print()\n",
    "    \n",
    "    cluster_reviews = frame[frame.cluster==i].review.tolist()\n",
    "    print('Cluster' + str(i) + 'reviews (' +str(len(cluster_reviews)) + 'reviews): ')\n",
    "    #print(', '.join(cluster_reviews))\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Latent Dirichlet Allocation(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model I used is Latent Dirichlet Allocation(LDA). (Blei, Ng, Jordan, 2003) LDA is a generative statistical model. In LDA, each document is assumed to be characterized by a particular set of topics. And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.\n",
    "\n",
    "I used LDA model in scikit-learn package to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method = 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 2998 reviews and 254 terms.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#We have to calculate tfidf here again because LDA requires integer values\n",
    "tfidf_model_lda = CountVectorizer(max_df = 0.99, max_features = 1000,\n",
    "                            min_df = 0.01, stop_words = 'english',\n",
    "                            tokenizer = tokenization_and_stemming, ngram_range = (1,1))\n",
    "\n",
    "tfidf_matrix_lda = tfidf_model_lda.fit_transform(data)\n",
    "\n",
    "print('In total, there are ' + str(tfidf_matrix_lda.shape[0]) + \\\n",
    "     ' reviews and ' + str(tfidf_matrix_lda.shape[1]) + ' terms.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2998, 5)\n",
      "[[0.57532313 0.02541305 0.0252892  0.3485363  0.02543832]\n",
      " [0.0515602  0.05128072 0.050988   0.42665331 0.41951777]\n",
      " [0.10310746 0.10063256 0.59625898 0.10000059 0.10000041]\n",
      " ...\n",
      " [0.37252261 0.03017967 0.02875373 0.53858721 0.02995678]\n",
      " [0.35692585 0.19473997 0.43199315 0.00809114 0.00824989]\n",
      " [0.02890742 0.29396788 0.02865587 0.02893855 0.61953027]]\n"
     ]
    }
   ],
   "source": [
    "#Matrix for reviews and topics. The review will be classified to the topic with highest score.\n",
    "lda_output = lda.fit_transform(tfidf_matrix_lda)\n",
    "print(lda_output.shape)\n",
    "print(lda_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 254)\n",
      "[[2.67862852e+01 6.72555480e+01 1.18030048e+01 ... 4.78508189e+01\n",
      "  7.09875333e+00 1.61578831e+02]\n",
      " [8.88485400e+01 1.12394452e+02 2.07434786e-01 ... 1.70418548e+01\n",
      "  3.88118187e+01 3.02901604e-01]\n",
      " [1.33998191e+00 9.26421513e+01 4.55690490e+01 ... 2.73071910e-01\n",
      "  6.15428648e+01 5.70111345e+00]\n",
      " [3.11400300e-01 1.51714903e+02 2.07386942e-01 ... 2.04374505e-01\n",
      "  1.57469246e+02 2.05643910e-01]\n",
      " [5.06509608e+01 2.50506598e+02 5.23825283e+00 ... 2.06408097e-01\n",
      "  3.22306022e+00 3.93780772e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Matrix for word and topics.\n",
    "topic_word = lda.components_\n",
    "print(topic_word.shape)\n",
    "print(topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc6</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc7</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.53</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc8</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic0  topic1  topic2  topic3  topic4  topic\n",
       "Doc0    0.58    0.03    0.03    0.35    0.03      0\n",
       "Doc1    0.05    0.05    0.05    0.43    0.42      3\n",
       "Doc2    0.10    0.10    0.60    0.10    0.10      2\n",
       "Doc3    0.88    0.03    0.03    0.03    0.03      0\n",
       "Doc4    0.07    0.19    0.33    0.10    0.30      2\n",
       "Doc5    0.03    0.03    0.03    0.18    0.73      4\n",
       "Doc6    0.22    0.57    0.03    0.15    0.03      1\n",
       "Doc7    0.03    0.39    0.03    0.03    0.53      4\n",
       "Doc8    0.59    0.32    0.01    0.07    0.01      0\n",
       "Doc9    0.02    0.25    0.37    0.02    0.34      2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check 10 examples and see how they are classified\n",
    "topic_names = ['topic' + str(i) for i in range(lda.n_components)]\n",
    "\n",
    "doc_names = ['Doc' + str(i) for i in range(len(data))]\n",
    "\n",
    "df_doc_topic = pd.DataFrame(np.round(lda_output, 2), columns = topic_names, index = doc_names)\n",
    "\n",
    "topic = np.argmax(df_doc_topic.values, axis = 1)\n",
    "df_doc_topic['topic'] = topic\n",
    "\n",
    "df_doc_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic\n",
       "4    734\n",
       "1    692\n",
       "0    653\n",
       "3    647\n",
       "2    272"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The result of LDA model. The result of LDA is more balanced than that of K-Means.\n",
    "df_doc_topic['topic'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.67862852e+01 6.72555480e+01 1.18030048e+01 ... 4.78508189e+01\n",
      "  7.09875333e+00 1.61578831e+02]\n",
      " [8.88485400e+01 1.12394452e+02 2.07434786e-01 ... 1.70418548e+01\n",
      "  3.88118187e+01 3.02901604e-01]\n",
      " [1.33998191e+00 9.26421513e+01 4.55690490e+01 ... 2.73071910e-01\n",
      "  6.15428648e+01 5.70111345e+00]\n",
      " [3.11400300e-01 1.51714903e+02 2.07386942e-01 ... 2.04374505e-01\n",
      "  1.57469246e+02 2.05643910e-01]\n",
      " [5.06509608e+01 2.50506598e+02 5.23825283e+00 ... 2.06408097e-01\n",
      "  3.22306022e+00 3.93780772e+00]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'m</th>\n",
       "      <th>'s</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accur</th>\n",
       "      <th>actual</th>\n",
       "      <th>adjust</th>\n",
       "      <th>alarm</th>\n",
       "      <th>alreadi</th>\n",
       "      <th>alway</th>\n",
       "      <th>...</th>\n",
       "      <th>weight</th>\n",
       "      <th>went</th>\n",
       "      <th>white</th>\n",
       "      <th>wife</th>\n",
       "      <th>wish</th>\n",
       "      <th>work</th>\n",
       "      <th>worn</th>\n",
       "      <th>worth</th>\n",
       "      <th>wrist</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>26.786285</td>\n",
       "      <td>67.255548</td>\n",
       "      <td>11.803005</td>\n",
       "      <td>0.204514</td>\n",
       "      <td>40.633106</td>\n",
       "      <td>9.672277</td>\n",
       "      <td>5.126643</td>\n",
       "      <td>75.179255</td>\n",
       "      <td>0.214948</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>...</td>\n",
       "      <td>17.217024</td>\n",
       "      <td>31.395853</td>\n",
       "      <td>32.391252</td>\n",
       "      <td>51.391098</td>\n",
       "      <td>4.766870</td>\n",
       "      <td>288.591649</td>\n",
       "      <td>26.779261</td>\n",
       "      <td>47.850819</td>\n",
       "      <td>7.098753</td>\n",
       "      <td>161.578831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>88.848540</td>\n",
       "      <td>112.394452</td>\n",
       "      <td>0.207435</td>\n",
       "      <td>0.203379</td>\n",
       "      <td>0.203366</td>\n",
       "      <td>42.193382</td>\n",
       "      <td>4.686924</td>\n",
       "      <td>0.204028</td>\n",
       "      <td>35.794780</td>\n",
       "      <td>16.795059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211113</td>\n",
       "      <td>0.203595</td>\n",
       "      <td>0.205318</td>\n",
       "      <td>0.202722</td>\n",
       "      <td>15.185887</td>\n",
       "      <td>63.110421</td>\n",
       "      <td>0.205802</td>\n",
       "      <td>17.041855</td>\n",
       "      <td>38.811819</td>\n",
       "      <td>0.302902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>1.339982</td>\n",
       "      <td>92.642151</td>\n",
       "      <td>45.569049</td>\n",
       "      <td>5.018191</td>\n",
       "      <td>6.775435</td>\n",
       "      <td>7.157278</td>\n",
       "      <td>29.285723</td>\n",
       "      <td>0.202628</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>1.943788</td>\n",
       "      <td>...</td>\n",
       "      <td>29.926114</td>\n",
       "      <td>0.204616</td>\n",
       "      <td>0.205484</td>\n",
       "      <td>0.202419</td>\n",
       "      <td>6.108349</td>\n",
       "      <td>0.784195</td>\n",
       "      <td>7.341350</td>\n",
       "      <td>0.273072</td>\n",
       "      <td>61.542865</td>\n",
       "      <td>5.701113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>0.311400</td>\n",
       "      <td>151.714903</td>\n",
       "      <td>0.207387</td>\n",
       "      <td>40.268551</td>\n",
       "      <td>0.202518</td>\n",
       "      <td>0.217836</td>\n",
       "      <td>26.679168</td>\n",
       "      <td>0.207334</td>\n",
       "      <td>0.204106</td>\n",
       "      <td>0.204431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210806</td>\n",
       "      <td>0.201933</td>\n",
       "      <td>0.204099</td>\n",
       "      <td>0.213153</td>\n",
       "      <td>10.466433</td>\n",
       "      <td>0.209171</td>\n",
       "      <td>0.205313</td>\n",
       "      <td>0.204375</td>\n",
       "      <td>157.469246</td>\n",
       "      <td>0.205644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic4</th>\n",
       "      <td>50.650961</td>\n",
       "      <td>250.506598</td>\n",
       "      <td>5.238253</td>\n",
       "      <td>0.397511</td>\n",
       "      <td>0.255457</td>\n",
       "      <td>0.255945</td>\n",
       "      <td>21.278080</td>\n",
       "      <td>4.332582</td>\n",
       "      <td>0.206222</td>\n",
       "      <td>32.655048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206769</td>\n",
       "      <td>4.858831</td>\n",
       "      <td>0.230998</td>\n",
       "      <td>0.204060</td>\n",
       "      <td>23.531060</td>\n",
       "      <td>90.746925</td>\n",
       "      <td>0.207232</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>3.223060</td>\n",
       "      <td>3.937808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               'm          's        abl    absolut      accur     actual  \\\n",
       "topic0  26.786285   67.255548  11.803005   0.204514  40.633106   9.672277   \n",
       "topic1  88.848540  112.394452   0.207435   0.203379   0.203366  42.193382   \n",
       "topic2   1.339982   92.642151  45.569049   5.018191   6.775435   7.157278   \n",
       "topic3   0.311400  151.714903   0.207387  40.268551   0.202518   0.217836   \n",
       "topic4  50.650961  250.506598   5.238253   0.397511   0.255457   0.255945   \n",
       "\n",
       "           adjust      alarm    alreadi      alway     ...         weight  \\\n",
       "topic0   5.126643  75.179255   0.214948   0.205408     ...      17.217024   \n",
       "topic1   4.686924   0.204028  35.794780  16.795059     ...       0.211113   \n",
       "topic2  29.285723   0.202628   0.205021   1.943788     ...      29.926114   \n",
       "topic3  26.679168   0.207334   0.204106   0.204431     ...       0.210806   \n",
       "topic4  21.278080   4.332582   0.206222  32.655048     ...       0.206769   \n",
       "\n",
       "             went      white       wife       wish        work       worn  \\\n",
       "topic0  31.395853  32.391252  51.391098   4.766870  288.591649  26.779261   \n",
       "topic1   0.203595   0.205318   0.202722  15.185887   63.110421   0.205802   \n",
       "topic2   0.204616   0.205484   0.202419   6.108349    0.784195   7.341350   \n",
       "topic3   0.201933   0.204099   0.213153  10.466433    0.209171   0.205313   \n",
       "topic4   4.858831   0.230998   0.204060  23.531060   90.746925   0.207232   \n",
       "\n",
       "            worth       wrist        year  \n",
       "topic0  47.850819    7.098753  161.578831  \n",
       "topic1  17.041855   38.811819    0.302902  \n",
       "topic2   0.273072   61.542865    5.701113  \n",
       "topic3   0.204375  157.469246    0.205644  \n",
       "topic4   0.206408    3.223060    3.937808  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print topic & term matrix. Five topics and 254 terms, each term is assigned a \n",
    "print(lda.components_)\n",
    "# topic-term matrix\n",
    "df_topic_words = pd.DataFrame(lda.components_)\n",
    "\n",
    "# column and index\n",
    "df_topic_words.columns = tfidf_model_lda.get_feature_names()\n",
    "df_topic_words.index = topic_names\n",
    "\n",
    "df_topic_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>watch</td>\n",
       "      <td>work</td>\n",
       "      <td>year</td>\n",
       "      <td>got</td>\n",
       "      <td>time</td>\n",
       "      <td>batteri</td>\n",
       "      <td>light</td>\n",
       "      <td>excel</td>\n",
       "      <td>n't</td>\n",
       "      <td>replac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>watch</td>\n",
       "      <td>great</td>\n",
       "      <td>look</td>\n",
       "      <td>like</td>\n",
       "      <td>n't</td>\n",
       "      <td>price</td>\n",
       "      <td>face</td>\n",
       "      <td>time</td>\n",
       "      <td>littl</td>\n",
       "      <td>cheap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>br</td>\n",
       "      <td>watch</td>\n",
       "      <td>hand</td>\n",
       "      <td>band</td>\n",
       "      <td>use</td>\n",
       "      <td>time</td>\n",
       "      <td>link</td>\n",
       "      <td>second</td>\n",
       "      <td>'s</td>\n",
       "      <td>n't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>watch</td>\n",
       "      <td>love</td>\n",
       "      <td>nice</td>\n",
       "      <td>perfect</td>\n",
       "      <td>small</td>\n",
       "      <td>fit</td>\n",
       "      <td>wrist</td>\n",
       "      <td>'s</td>\n",
       "      <td>band</td>\n",
       "      <td>easi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>watch</td>\n",
       "      <td>good</td>\n",
       "      <td>look</td>\n",
       "      <td>'s</td>\n",
       "      <td>time</td>\n",
       "      <td>qualiti</td>\n",
       "      <td>n't</td>\n",
       "      <td>set</td>\n",
       "      <td>need</td>\n",
       "      <td>beauti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 0 Word 1 Word 2   Word 3 Word 4   Word 5 Word 6  Word 7 Word 8  \\\n",
       "Topic 0  watch   work   year      got   time  batteri  light   excel    n't   \n",
       "Topic 1  watch  great   look     like    n't    price   face    time  littl   \n",
       "Topic 2     br  watch   hand     band    use     time   link  second     's   \n",
       "Topic 3  watch   love   nice  perfect  small      fit  wrist      's   band   \n",
       "Topic 4  watch   good   look       's   time  qualiti    n't     set   need   \n",
       "\n",
       "         Word 9  \n",
       "Topic 0  replac  \n",
       "Topic 1   cheap  \n",
       "Topic 2     n't  \n",
       "Topic 3    easi  \n",
       "Topic 4  beauti  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print top n keywords for each topic \n",
    "def print_topic_words(tfidf_model, lda_model, n_words):\n",
    "    words = np.array(tfidf_model.get_feature_names())\n",
    "    topic_words = []\n",
    "    for topic_words_weights in lda_model.components_:\n",
    "        top_words = topic_words_weights.argsort()[::-1][:n_words]\n",
    "        topic_words.append(words.take(top_words))\n",
    "    return topic_words\n",
    "\n",
    "topic_keywords = print_topic_words(tfidf_model = tfidf_model_lda, lda_model = lda, n_words = 10) \n",
    "\n",
    "df_topic_words = pd.DataFrame(topic_keywords)\n",
    "df_topic_words.columns = ['Word ' + str(i) for i in range(df_topic_words.shape[1])]\n",
    "df_topic_words.index = ['Topic ' + str(i) for i in range(df_topic_words.shape[0])]\n",
    "df_topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I chose two unsupervised learning models--K-Means and LDA to do topic modeling on a watch user reviews dataset. The result of LDA model is much more balanced than that of K-Means Clustering model. So LDA performed better than K-Means.\n",
    "\n",
    "In addition, if we look at the top 10 words of each topic, as the printed table above, we can see that the classification makes sense. The main topic is watch, of course. But each topic contains different details other than watch: apperance, watch band, price, quality and watch as a gift. Note that the order of these five detailed topics may be different everyime you run the code and retrain the model. But it is definite that each one will contain one of these five detailed topics.\n",
    "\n",
    "The differences of the topic modeling results are not that obvious because the dataset I chose is all about watch reviews. It will show larger differences if the dataset contains documents that are more different in topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I used a dataset all about watch user reviews. There are many limits due to the dataset contains reviews that has one similar main topic: watch. In the future, I want to use dataset about wikipedia title or academic article paper title and abstract to do LDA topic modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Rachael Tatman, Data Science 101, 2017\n",
    "\n",
    "[2] Blei, Ng, Jordan, Latent Dirichlet, 2003\n",
    "\n",
    "[3] MacQueen, Some Methods for Classification and Analysis of Multivariate Observations, 1967."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
